---
name: pentest_reporting
description: Learn to document assessments professionally — executive summaries, technical findings with CVSS 4.0/DREAD scoring, evidence collection and chain of custody, remediation roadmaps, MITRE ATT&CK mapping, and compliance reporting for PCI DSS 4.0, SOC 2, and HIPAA.
---
# Penetration Testing Reporting

## When to Use This Skill

- Documenting findings from a penetration test or red team engagement
- Writing executive summaries for non-technical stakeholders
- Scoring vulnerabilities using CVSS 4.0 or DREAD methodologies
- Preparing compliance-specific reports (PCI DSS, SOC 2, HIPAA)
- Exporting data from Metasploit for report generation
- Creating attack narratives with MITRE ATT&CK mapping

---

## Concepts

### Report Audiences

| Audience | Focus | Language | Sections |
|----------|-------|----------|----------|
| Executives/Board | Business risk, strategic impact | Non-technical | Executive summary, remediation roadmap |
| Technical leads | Prioritization, resource allocation | Mixed | Findings summary, remediation steps |
| Engineers | Reproduction, fixes | Technical | Detailed findings, evidence, PoC |
| Auditors | Control validation, compliance | Compliance-focused | Scope, methodology, evidence |

### Risk Communication Principles

- **Translate technical to business** — "SQL injection" means nothing to executives; "attackers can steal all customer data" does
- **Quantify impact** — Reference breach costs ($9.48M average US breach per IBM 2024), regulatory fines, reputational damage
- **Prioritize actionably** — Don't just rank by CVSS; consider business context, exploitability, and asset criticality
- **Be specific** — Vague findings ("improve security") waste time; actionable findings drive remediation

---

## Report Types

| Type | Purpose | Key Differences |
|------|---------|-----------------|
| Vulnerability Assessment | Identify weaknesses | Scanning-focused, breadth over depth |
| Penetration Test | Validate exploitability | Proves impact through exploitation |
| Red Team Assessment | Test detection/response | Attack narrative, stealth focus, kill chain |
| Purple Team Report | Improve detection | Defense gaps, detection recommendations |
| Bug Bounty Submission | Single vulnerability | Concise, reproducible, impact-focused |

---

## Report Structure

### Executive Summary

**Purpose**: Enable decision-makers to understand risk and authorize resources. Length: 1-2 pages maximum.

**Required Elements**:
1. **Engagement overview** — Scope, dates, methodology (1-2 sentences)
2. **Overall risk posture** — Critical/High/Medium/Low summary
3. **Key findings** — Top 3-5 critical issues in plain language
4. **Business impact** — What attackers could achieve, potential costs
5. **Strategic recommendations** — Prioritized actions for leadership

**Writing Guidelines**:
- No jargon or acronyms without explanation
- Focus on "what could happen" not "what we found"
- Use charts/visuals for severity distribution
- End with clear call to action

**Example Opening**:
> During the assessment conducted November 1-15, 2024, we identified 3 critical, 7 high, and 12 medium-severity vulnerabilities. An attacker exploiting the critical findings could gain complete control of the customer database, affecting 50,000 user records and potentially triggering GDPR notification requirements.

### Scope and Methodology

**Document**:
- Target systems/networks (IP ranges, URLs, applications)
- Testing methodology (OWASP, PTES, OSSTMM, custom)
- Testing dates and duration
- Tools used (list major tools)
- Limitations and exclusions
- Out-of-scope items
- Rules of engagement (hours, no-test systems)

### Findings Summary

**Visual Elements**:
- Severity distribution chart (bar or donut, not pie)
- Finding categories (web, network, configuration, etc.)
- Trend analysis for recurring assessments
- Industry benchmark comparison if available

**Table Format**:
```
| ID | Finding | Severity | CVSS | Status |
|----|---------|----------|------|--------|
| F1 | SQL Injection in login | Critical | 9.8 | Open |
| F2 | Weak TLS configuration | Medium | 5.3 | Open |
```

### Attack Narrative (Red Team)

For red team engagements, include a story-based presentation:

1. **Timeline** — Chronological attack progression
2. **Initial access** — How entry was achieved
3. **Privilege escalation** — Path to elevated access
4. **Lateral movement** — Progression through network
5. **Objective completion** — Crown jewels accessed
6. **Detection gaps** — What security missed
7. **MITRE ATT&CK mapping** — Technique IDs for each phase

**Example**:
> Day 1: Initial access via phishing [T1566.001] delivered malicious macro [T1204.002]. Day 2: Credential harvesting from LSASS [T1003.001] yielded domain admin hash. Day 3: Lateral movement via PsExec [T1021.002] to finance server. Objective: Full access to ERP system achieved without detection.

### Remediation Roadmap

**Structure by effort/impact**:

| Priority | Timeframe | Findings | Effort |
|----------|-----------|----------|--------|
| Immediate | 0-7 days | F1, F3 | Low |
| Short-term | 1-4 weeks | F2, F5, F7 | Medium |
| Medium-term | 1-3 months | F4, F8 | High |
| Long-term | 3-6 months | F6, F9 | High |

**Include**:
- Dependencies between fixes
- Resource requirements (personnel, tools, budget)
- Verification criteria for each fix
- Risk acceptance options for delayed items

### Appendices

- Raw tool output (sanitized)
- Full screenshots
- Evidence files with hashes
- Detailed technical reproduction steps
- Tool configurations used

---

## Finding Documentation

### Finding Template

```markdown
## F[ID]: [Descriptive Title]

**Severity**: Critical/High/Medium/Low
**CVSS 4.0 Score**: X.X (Vector String)
**Affected Systems**: [IP/URL/Application]
**CWE**: CWE-XXX
**Status**: Open/Remediated/Accepted Risk

### Description
[What the vulnerability is, why it exists, and why it matters]

### Evidence
[Screenshots, commands, responses with timestamps]

### Impact
[Specific business/technical impact if exploited]

### Steps to Reproduce
1. Step one
2. Step two
3. ...

### Remediation
[Specific, actionable fix — not "patch the system"]

### References
- CVE-XXXX-XXXXX
- Vendor advisory URL
- OWASP reference
```

### Writing Good Findings

**Be Specific**:
- Bad: "SQL injection found in the application"
- Good: "SQL injection in `/api/users?id=` parameter allows unauthenticated extraction of all user credentials via UNION-based injection"

**Include Business Context**:
- Bad: "CVSS 9.8, Critical"
- Good: "This vulnerability enables complete database extraction, affecting 50,000 customer records including PII"

**Provide Actionable Remediation**:
- Bad: "Use parameterized queries"
- Good: "Replace line 47 in `UserController.php`: use PDO prepared statements with bound parameters. Example: `$stmt = $pdo->prepare('SELECT * FROM users WHERE id = ?');`"

---

## Severity and Risk Rating

### CVSS 4.0

CVSS 4.0 (released November 2023) replaces CVSS 3.1 with improved granularity.

**Score Types**:
- **CVSS-B**: Base metrics only
- **CVSS-BT**: Base + Threat metrics
- **CVSS-BE**: Base + Environmental metrics
- **CVSS-BTE**: All metrics (most accurate)

**Base Metrics**:
| Metric | Values |
|--------|--------|
| Attack Vector | Network/Adjacent/Local/Physical |
| Attack Complexity | Low/High |
| Attack Requirements | None/Present (new in 4.0) |
| Privileges Required | None/Low/High |
| User Interaction | None/Passive/Active (expanded) |
| Vulnerable System Impact | C/I/A: None/Low/High |
| Subsequent System Impact | C/I/A: None/Low/High (replaces Scope) |

**Severity Ratings**:
| Score | Severity |
|-------|----------|
| 0.0 | None |
| 0.1-3.9 | Low |
| 4.0-6.9 | Medium |
| 7.0-8.9 | High |
| 9.0-10.0 | Critical |

**Calculator**: Use FIRST's official calculator at first.org/cvss/calculator/4.0

### DREAD Model

DREAD provides qualitative scoring when CVSS doesn't fit (custom vulnerabilities, business logic flaws):

| Factor | Question | Score |
|--------|----------|-------|
| **D**amage | How severe if exploited? | 1-10 |
| **R**eproducibility | How easy to reproduce? | 1-10 |
| **E**xploitability | How easy to exploit? | 1-10 |
| **A**ffected Users | How many users impacted? | 1-10 |
| **D**iscoverability | How easy to find? | 1-10 |

**Calculation**: `Risk = (D + R + E + A + D) / 5`

**Severity Mapping**:
| Score | Severity |
|-------|----------|
| 1-10 | Low |
| 11-24 | Medium |
| 25-39 | High |
| 40-50 | Critical |

### Beyond CVSS: Contextual Prioritization

CVSS alone is insufficient. Consider:

- **Exploitability**: Is there a public exploit? (Check EPSS scores)
- **Asset Criticality**: Is this a crown jewel system?
- **Exposure**: Internet-facing vs internal?
- **Compensating Controls**: What mitigates the risk?
- **Chained Vulnerabilities**: Medium + Medium can equal Critical

---

## Evidence Management

### Collection Standards

**Screenshots**:
- Include full window with URL/terminal prompt visible
- Add timestamps (system clock visible or annotation)
- Capture before and after states
- Save originals with metadata intact
- Use lossless formats (PNG, not JPEG)

**Commands and Output**:
```bash
# Document with timestamp
date && whoami && id
# Full command with context
curl -X POST http://target/api/login -d "user=admin' OR '1'='1" -v
```

**Network Captures**:
- Filter to relevant traffic only
- Store as PCAP, not screenshots
- Document capture parameters
- Redact sensitive unrelated data

### Chain of Custody

For evidence that may be used legally:

```
Evidence ID: EVD-2024-001
Description: LSASS memory dump showing cleartext credentials
Collected By: [Tester Name]
Collection Date/Time: 2024-11-15 14:32:00 UTC
Collection Method: Procdump via Meterpreter
Hash (SHA-256): a1b2c3d4e5...
Storage Location: Encrypted evidence drive, locker 3
Access Log:
  - 2024-11-15 14:35 - [Tester] - Initial storage
  - 2024-11-16 09:00 - [Reviewer] - Verification
```

**Best Practices**:
- Hash all evidence files immediately (SHA-256)
- Never work with originals — copy first
- Document all transfers with signatures
- Use encrypted storage
- Maintain access log

### Retention Periods

| Standard | Requirement |
|----------|-------------|
| PCI DSS | 12 months minimum |
| HIPAA | 6 years |
| SOC 2 | Audit period + 1 year typical |
| General | 3-7 years recommended |

---

## Metasploit Data Export

### Database Queries for Reporting

```
msf6 > hosts -c address,name,os_name,os_flavor,purpose
msf6 > hosts -o /path/to/hosts.csv

msf6 > services -c port,proto,name,info,state
msf6 > services -p 445 -c port,name,info -o smb_services.csv

msf6 > vulns -c host,name,refs
msf6 > vulns -S ms17 -o ms17_vulns.csv

msf6 > creds -c host,port,user,pass,type
msf6 > creds -o credentials.csv

msf6 > loot -c host,type,name,path
msf6 > notes -c host,type,data
```

### Full Database Export

```
msf6 > db_export -f xml /path/to/engagement_export.xml
```

Exports: hosts, events, services, credentials, web sites, web pages, web forms, web vulns

### Session and Activity Logs

```
msf6 > spool /path/to/session_log.txt
# ... perform activities ...
msf6 > spool off
```

---

## Compliance-Specific Reporting

### PCI DSS 4.0 (Requirement 11.4)

**Required Documentation**:
- Penetration testing methodology documented
- Application-layer testing (OWASP Top 10 vulnerabilities)
- Network-layer testing (all supporting components)
- Review of threats/vulnerabilities from past 12 months
- Risk assessment for each finding
- Remediation verification (retest after fixes)

**Frequency**: Annual minimum; every 6 months for service providers

**Retention**: 12 months minimum

**Segmentation Testing**: If CDE isolation used, test segmentation controls separately

### SOC 2 (CC4.1, CC7.1)

**Evidence Needed**:
- Pentest report with methodology
- Finding severity and remediation status
- Timeline showing test within audit period
- Proof of remediation for identified issues

**Mapping**: Reference specific Trust Services Criteria (CC4.1 Monitoring, CC7.1 Vulnerability Management)

### HIPAA (§ 164.308(a)(8))

**Requirements** (proposed 2025 updates):
- Penetration testing at least annually
- Vulnerability scanning every 6 months
- Written documentation of all findings
- Remediation plan with responsible parties
- Technology asset inventory
- Network map showing ePHI flow

**Focus Areas**: Administrative, physical, and technical safeguards for ePHI

---

## MITRE ATT&CK Mapping

### Integration in Reports

Map findings to ATT&CK techniques for standardized communication:

```
Finding: Kerberoasting successful, 3 service account hashes cracked
ATT&CK Mapping:
  - Tactic: Credential Access [TA0006]
  - Technique: Steal or Forge Kerberos Tickets [T1558]
  - Sub-technique: Kerberoasting [T1558.003]
```

### Attack Path Visualization

Use ATT&CK Navigator to create visual attack path maps:
1. Document each technique used in engagement
2. Create navigator layer JSON
3. Export as SVG/PNG for report inclusion
4. Color-code by detection status (detected/missed)

### Benefits

- Common language between red and blue teams
- Maps to defensive controls
- Enables gap analysis
- Supports purple team exercises

---

## Tools

### Reporting Platforms

| Tool | Type | Key Features |
|------|------|--------------|
| **Dradis** | Commercial/Open | 24 tool integrations, custom templates, team collaboration |
| **PlexTrac** | Commercial | AI-assisted, compliance mapping, client portal |
| **Serpico** | Open Source | Template-based, Nessus/Burp import, multi-user |
| **SysReptor** | Open Source | Modern UI, markdown support, collaborative |
| **Ghostwriter** | Open Source | Red team focused, infrastructure tracking |

### Template Formats

- **Word**: Most client-compatible, supports complex formatting
- **Markdown**: Version control friendly, converts to multiple formats
- **LaTeX**: Professional typesetting, reproducible builds
- **HTML**: Interactive elements, web delivery

---

## Writing Guidelines

### Tone and Style

- **Professional and objective** — State facts, not opinions
- **Active voice** — "The tester discovered" not "It was discovered"
- **Consistent terminology** — Pick terms and stick with them
- **No jargon without explanation** — Define acronyms on first use
- **Vendor-neutral recommendations** — Avoid product pitches

### Common Mistakes to Avoid

- Pie charts for severity (implies completeness)
- Generic remediation advice ("patch systems")
- Missing reproduction steps
- Screenshots without context
- Findings without business impact
- Executive summary that's too technical

---

## Workflows

### Evidence Collection Workflow

1. **Before testing**: Create evidence directory structure, prepare templates
2. **During testing**: Screenshot immediately, log commands with timestamps
3. **After testing**: Hash all files, organize by finding ID
4. **Review**: Verify all findings have supporting evidence
5. **Storage**: Encrypted archive with documented retention

### Finding Documentation Workflow

1. **Discovery**: Note finding during testing with preliminary details
2. **Validation**: Confirm exploitability, gather evidence
3. **Scoring**: Calculate CVSS 4.0, consider context adjustments
4. **Drafting**: Write full finding using template
5. **Review**: Peer review for accuracy and clarity
6. **Finalization**: Add to report, update summary tables

### Report Assembly Workflow

1. **Export data**: Metasploit db_export, tool outputs
2. **Draft findings**: Complete all finding documentation
3. **Build summary**: Aggregate statistics, create charts
4. **Write narrative**: Executive summary and methodology
5. **Peer review**: Technical accuracy and readability
6. **QA check**: Spelling, formatting, evidence verification
7. **Client delivery**: Secure transmission, presentation prep

### Retest Workflow

1. **Receive remediation notice** from client
2. **Verify scope**: Which findings are claimed fixed
3. **Retest**: Attempt original exploitation
4. **Document**: Pass/fail with evidence
5. **Update report**: Change status, add retest notes
6. **Deliver**: Updated report or retest addendum
